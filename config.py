# config.py
import os
from dataclasses import dataclass


# =========================================================
# Backend: API settings (OpenAI package -> your vLLM endpoints)
# =========================================================
@dataclass
class APIConfig:
    """
    Use OpenAI python package but point base_url to your local vLLM servers.

    Environment variables:
      - OPENAI_API_KEY: can be dummy for vLLM, but required by OpenAI client
      - LLM_BASE_URL: e.g. http://localhost:8000/v1
      - EMB_BASE_URL: e.g. http://localhost:8001/v1
      - LLM_MODEL: model name your vLLM exposes
      - EMB_MODEL: embedding model name your embedding server exposes
    """
    API_KEY: str = os.getenv("OPENAI_API_KEY", "EMPTY")

    LLM_BASE_URL: str = os.getenv("LLM_BASE_URL", "http://localhost:8000/v1")
    EMB_BASE_URL: str = os.getenv("EMB_BASE_URL", "http://localhost:8002/v1")

    LLM_MODEL: str = os.getenv("LLM_MODEL", "Qwen/Qwen3-4B-Instruct-2507-FP8")
    EMB_MODEL: str = os.getenv("EMB_MODEL", "Qwen/Qwen3-Embedding-0.6B")


# =========================================================
# Backend: storage settings (per-user isolated memory)
# =========================================================
@dataclass
class StoreConfig:
    """
    Storage layout:
      STORAGE_DIR/
        users/
          <user_id_sanitized>/
            faiss.index
            meta.jsonl
            state.json
            conversations/
              <conv_id>.jsonl

    Environment variables:
      - STORAGE_DIR: default ./storage
      - EMBEDDING_DIMS: 0 means infer from first embedding vector
    """
    STORAGE_DIR: str = os.getenv("STORAGE_DIR", "./storage")
    USERS_DIR: str = os.path.join(STORAGE_DIR, "users")
    EMBEDDING_DIMS: int = int(os.getenv("EMBEDDING_DIMS", "0"))


# =========================================================
# Backend: prompt templates
# =========================================================
@dataclass
class PromptConfig:
    """
    You can tune these prompts freely.
    Keep outputs JSON-only for gate/rerank to make parsing robust.
    """

    # Main assistant system prompt
    CHAT_SYSTEM: str = (
        "You are a helpful assistant.\n"
        "You may receive retrieved memory in two forms:\n"
        "1) Replay conversations: full multi-turn user/assistant exchanges.\n"
        "2) Summary-only memory: short bullet summaries in system messages.\n"
        "Use them only if relevant and do not hallucinate facts."
    )

    # Summarize each (user, assistant) turn into a memory snippet
    SUMMARY_SYSTEM: str = (
        "You are an assistant that writes compact, factual memory summaries.\n"
        "Write 1-3 sentences capturing durable info, decisions, preferences, and key facts.\n"
        "Avoid ephemeral details unless they matter.\n"
        "Return only the summary text."
    )

    # Gate: keep or drop candidates (NOT ranking)
    GATE_SYSTEM: str = (
        "You are a strict relevance filter.\n"
        "Given a QUERY and a list of CANDIDATES (each with an id and a summary),\n"
        "decide which candidates are relevant enough to keep.\n"
        "\n"
        "Output JSON ONLY with fields:\n"
        "  keep_ids: list of kept candidate ids (ordered by your confidence, most relevant first)\n"
        "  drop_ids: list of dropped candidate ids\n"
        "  reasons: a dict mapping id -> short reason (optional)\n"
        "\n"
        "Rules:\n"
        "- Keep if it directly helps answer the query or provides necessary context.\n"
        "- Drop if irrelevant, too generic, or about a different topic.\n"
        "- Prefer precision over recall."
    )

    # Rerank: pick top-K among gated candidates for replay
    RERANK_SYSTEM: str = (
        "You are a reranker.\n"
        "Given QUERY and CANDIDATES (id + summary), select the best ones to replay as full conversations.\n"
        "\n"
        "Output JSON ONLY with field:\n"
        "  top_ids: list of candidate ids in descending usefulness.\n"
        "\n"
        "Ranking criteria:\n"
        "- Most directly answers or supports the query.\n"
        "- Contains concrete decisions, preferences, constraints, or prior reasoning.\n"
        "- Avoid duplicates and overly generic memories."
    )


# =========================================================
# Frontend: runtime config (backend URL NOT editable in GUI)
# =========================================================
@dataclass
class FrontendRuntimeConfig:
    """
    Frontend reads BACKEND_URL at startup:
      export BACKEND_URL="http://localhost:9000/v1/chat/completions"
    """
    BACKEND_URL: str = os.getenv("BACKEND_URL", "http://localhost:9000/v1/chat/completions")


# =========================================================
# Frontend: default parameter values
# =========================================================
@dataclass
class UIParamDefaults:
    recent_turns: int = 6
    candidate_n: int = 60
    retrieve_k: int = 5
    use_gate: bool = True
    gate_keep_max: int = 30
    use_rerank: bool = True
    temperature: float = 0.3
    max_tokens: int = 768


# =========================================================
# Frontend: i18n UI text (extendable)
# =========================================================
UI_TEXT = {
    "EN": {
        "app_title": "ğŸ’¬ Persistent Memory Chatbot",
        "sidebar": {
            "language": "Language",

            "identity_title": "Identity",
            "user_id": "User ID",
            "user_id_help": (
                "Used to isolate memory across different users. "
                "Keep it stable to reuse memory across sessions."
            ),
            "conv_id": "Conversation ID",
            "conv_id_help": (
                "Identifies a chat thread (recent window is per conversation). "
                "Click â€œNew chatâ€ to start a fresh thread."
            ),
            "new_chat": "New chat",
            "clear_ui": "Clear UI only",
            "clear_ui_help": "Clears the frontend display only; backend memory is not deleted.",

            "context_title": "Context & Memory",
            "recent_turns": "Recent window (turns)",
            "recent_turns_help": (
                "Last N user+assistant turns in THIS conversation, injected as normal multi-turn context."
            ),
            "candidate_n": "Vector candidates (N)",
            "candidate_n_help": (
                "How many memory summaries to retrieve from FAISS before filtering. "
                "Higher improves recall but increases gate/rerank cost."
            ),
            "retrieve_k": "Replay top-K",
            "retrieve_k_help": (
                "How many filtered memories are replayed as full multi-turn (original user+assistant)."
            ),
            "use_gate": "Enable gate",
            "use_gate_help": (
                "Binary filter: keep/drop candidate memories before reranking (NOT ranking)."
            ),
            "gate_keep_max": "Gate keep max",
            "gate_keep_max_help": (
                "Maximum candidates kept after gate. Non-replayed kept items go to summary-only system memory."
            ),
            "use_rerank": "Enable rerank",
            "use_rerank_help": (
                "Use an LLM reranker to select top-K for replay. If off, use first K gated candidates."
            ),

            "gen_title": "Generation",
            "temperature": "Temperature",
            "temperature_help": "Controls randomness. Lower is more deterministic.",
            "max_tokens": "Max tokens",
            "max_tokens_help": "Maximum output tokens for assistant reply.",
        },
        "main": {
            "input_placeholder": "Message",
            "thinking": "Thinking...",
            "history_used": "History used this turn",
            "replay_section": "Replay conversations (multi-turn)",
            "summary_section": "Additional memory (summaries only)",
            "none": "None.",
            "backend_error_tip": (
                "Tip: check backend logs, or ensure BACKEND_URL is correct. "
                "If the backend returns JSON traceback, it will be shown above."
            ),
        },
    },

    "ZH": {
        "app_title": "ğŸ’¬ å¯æŒä¹…åŒ–è®°å¿†å¯¹è¯æœºå™¨äºº",
        "sidebar": {
            "language": "è¯­è¨€",

            "identity_title": "èº«ä»½è®¾ç½®",
            "user_id": "ç”¨æˆ· ID",
            "user_id_help": (
                "ç”¨äºéš”ç¦»ä¸åŒç”¨æˆ·çš„è®°å¿†ç´¢å¼•ï¼ˆä¸åŒç”¨æˆ·ä¸ä¼šå…±äº«è®°å¿†ï¼‰ã€‚"
                "å¦‚æœå¸Œæœ›è·¨ä¼šè¯å¤ç”¨è®°å¿†ï¼Œè¯·ä¿æŒè¯¥ ID ç¨³å®šã€‚"
            ),
            "conv_id": "å¯¹è¯ ID",
            "conv_id_help": (
                "ç”¨äºæ ‡è¯†ä¸€ä¸ªå¯¹è¯çº¿ç¨‹ï¼ˆrecent window æŒ‰å¯¹è¯åŒºåˆ†ï¼‰ã€‚"
                "ç‚¹å‡»â€œæ–°å¯¹è¯â€ä¼šåˆ›å»ºæ–°çš„å¯¹è¯çº¿ç¨‹ã€‚"
            ),
            "new_chat": "æ–°å¯¹è¯",
            "clear_ui": "ä»…æ¸…ç©ºç•Œé¢",
            "clear_ui_help": "åªæ¸…ç©ºå‰ç«¯æ˜¾ç¤ºï¼Œä¸ä¼šåˆ é™¤åç«¯å·²å†™å…¥çš„è®°å¿†ã€‚",

            "context_title": "ä¸Šä¸‹æ–‡ä¸è®°å¿†",
            "recent_turns": "æœ€è¿‘çª—å£ï¼ˆè½®æ•°ï¼‰",
            "recent_turns_help": (
                "å½“å‰å¯¹è¯çº¿ç¨‹å†…æœ€è¿‘ N è½® user+assistantï¼Œ"
                "ä»¥å¤šè½®å¯¹è¯å½¢å¼ç›´æ¥æ³¨å…¥æ¨¡å‹ä¸Šä¸‹æ–‡ã€‚"
            ),
            "candidate_n": "å‘é‡å€™é€‰æ•°ï¼ˆNï¼‰",
            "candidate_n_help": (
                "ä» FAISS å¬å› N æ¡å€™é€‰æ‘˜è¦ï¼Œå†è¿›è¡Œ Gate/é‡æ’ã€‚"
                "N è¶Šå¤§å¬å›ç‡æ›´é«˜ï¼Œä½† Gate/é‡æ’æˆæœ¬ä¹Ÿæ›´é«˜ã€‚"
            ),
            "retrieve_k": "å›æ”¾ Top-K",
            "retrieve_k_help": (
                "ä»è¿‡æ»¤åçš„å€™é€‰ä¸­é€‰å‡º Top-Kï¼Œä»¥â€œåŸå§‹å¤šè½®å¯¹è¯ï¼ˆuser+assistantï¼‰â€å½¢å¼å›æ”¾ç»™æ¨¡å‹ã€‚"
            ),
            "use_gate": "å¯ç”¨è¿‡æ»¤é—¨ï¼ˆGateï¼‰",
            "use_gate_help": (
                "äºŒåˆ†ç±»è¿‡æ»¤ï¼šåˆ¤æ–­å€™é€‰è®°å¿†æ˜¯å¦ä¸å½“å‰é—®é¢˜ç›¸å…³ï¼ˆä¿ç•™/ä¸¢å¼ƒï¼‰ï¼Œä¸æ˜¯æ’åºã€‚"
            ),
            "gate_keep_max": "Gate ä¿ç•™ä¸Šé™",
            "gate_keep_max_help": (
                "Gate åæœ€å¤šä¿ç•™å¤šå°‘æ¡å€™é€‰ã€‚"
                "å…¶ä¸­æœªè¢«å›æ”¾çš„éƒ¨åˆ†ä¼šä½œä¸ºâ€œæ‘˜è¦ï¼ˆsystemï¼‰è®°å¿†â€æ³¨å…¥ã€‚"
            ),
            "use_rerank": "å¯ç”¨é‡æ’ï¼ˆRerankï¼‰",
            "use_rerank_help": (
                "ä½¿ç”¨ LLM å¯¹ Gate åå€™é€‰é‡æ’ï¼Œé€‰æ‹© Top-K å›æ”¾ã€‚å…³é—­æ—¶ç›´æ¥å–å‰ K æ¡ã€‚"
            ),

            "gen_title": "ç”Ÿæˆå‚æ•°",
            "temperature": "æ¸©åº¦ï¼ˆTemperatureï¼‰",
            "temperature_help": "éšæœºæ€§æ§åˆ¶ï¼Œè¶Šä½è¶Šç¨³å®šã€‚",
            "max_tokens": "æœ€å¤§ç”Ÿæˆ Tokens",
            "max_tokens_help": "é™åˆ¶åŠ©æ‰‹å›å¤çš„æœ€å¤§ token æ•°ã€‚",
        },
        "main": {
            "input_placeholder": "è¾“å…¥æ¶ˆæ¯â€¦",
            "thinking": "æ€è€ƒä¸­â€¦",
            "history_used": "æœ¬è½®ä½¿ç”¨çš„å†å²ä¸Šä¸‹æ–‡",
            "replay_section": "å›æ”¾å¯¹è¯ï¼ˆå¤šè½®å½¢å¼ï¼‰",
            "summary_section": "è¡¥å……è®°å¿†ï¼ˆä»…æ‘˜è¦ï¼‰",
            "none": "æ— ã€‚",
            "backend_error_tip": (
                "æç¤ºï¼šè¯·æ£€æŸ¥åç«¯æ—¥å¿—ï¼Œæˆ–ç¡®è®¤ BACKEND_URL é…ç½®æ­£ç¡®ã€‚"
                "å¦‚æœåç«¯è¿”å› JSON tracebackï¼Œä¼šåœ¨ä¸Šæ–¹æ˜¾ç¤ºã€‚"
            ),
        },
    },
}
